# WASM Optimization & Parallelization Plan

## Current Architecture

### Client-Side (WASM)
```
User Input ‚Üí Preprocessing ‚Üí CKKS Encryption (Go WASM)
                                    ‚Üì
                            5 encrypted features
                                    ‚Üì
                            Backend (HTTP POST)
                                    ‚Üì
                            Encrypted Score
                                    ‚Üì
                            CKKS Decryption (Go WASM)
                                    ‚Üì
                            Display Result
```

### Performance Bottlenecks (Measured - Nov 2025)
1. **Encryption (Browser WASM)**: ~667ms (5 features √ó 133ms each)
2. **Network Transfer**: ~50ms (7.9MB ciphertext, ~11MB RLK)
3. **Backend Computation**: ~260ms (weighted sum + sigmoid + RLK deserialization)
4. **Decryption**: ~150ms
5. **Total**: ~1127ms (1.1 seconds)

**Note**: Measurements from real E2E tests with MaxLevel=5, LogN=14

---

## Optimization Strategies

### üöÄ Phase 1: WASM Parallelization (Q1 2026)

#### 1.1 Web Workers for Parallel Encryption
**Status**: ‚ùå **TESTED - NOT BENEFICIAL**

**Current**: Sequential encryption of 5 features
```javascript
// Sequential (measured: 667ms)
for (let i = 0; i < 5; i++) {
  features[i] = await encrypt(values[i]); // 133ms each ‚Üí 667ms total
}
```

**Tested with Web Workers**:
```javascript
// Parallel (measured: 667ms - NO IMPROVEMENT)
const workers = [worker1, worker2, worker3, worker4, worker5];
const promises = features.map((val, i) => 
  workers[i].postMessage({ cmd: 'encrypt', value: val })
);
await Promise.all(promises); // Still 667ms!
```

**Why Web Workers Don't Help:**
1. **WASM Loading Overhead**: Each worker loads 7.6MB WASM module (~400ms)
2. **Message Passing Cost**: postMessage serialization/deserialization
3. **Memory Copying**: No SharedArrayBuffer support for WASM memory
4. **Context Switching**: OS overhead for thread management
5. **Result**: Total overhead (400-500ms) ‚â• potential parallel gains

**Implementation Plan**:
```typescript
// frontend/src/workers/encryptWorker.ts
import wasmModule from '../services/wasmLoader';

self.onmessage = async (e) => {
  const { cmd, value, publicKey } = e.data;
  
  if (cmd === 'encrypt') {
    const encrypted = await wasmModule.encrypt(value, publicKey);
    self.postMessage({ result: encrypted });
  }
};

// frontend/src/services/parallelEncryption.ts
export async function encryptFeaturesParallel(
  features: number[], 
  publicKey: string
): Promise<string[]> {
  const workerPool = new Array(5)
    .fill(null)
    .map(() => new Worker(new URL('../workers/encryptWorker.ts', import.meta.url)));

  const promises = features.map((value, i) => 
    new Promise<string>((resolve) => {
      workerPool[i].postMessage({ cmd: 'encrypt', value, publicKey });
      workerPool[i].onmessage = (e) => resolve(e.data.result);
    })
  );

  const results = await Promise.all(promises);
  
  // Cleanup workers
  workerPool.forEach(w => w.terminate());
  
  return results;
}
```

**Actual Results (Benchmarked)**:
- Sequential: 667ms (baseline)
- Parallel (Web Workers): 667ms (1.00x - NO SPEEDUP)
- Overhead: ~400-500ms (worker init + WASM loading)
- **Conclusion**: ‚ùå NOT worth implementing in production

**Lessons Learned**:
- WASM operations are too fast relative to worker overhead
- Only beneficial for CPU-intensive JS operations, not WASM
- Better to focus on WASM-level optimizations (SIMD, etc.)

---

#### 1.2 SharedArrayBuffer for Zero-Copy Transfer
**Problem**: Transferring large ciphertexts (1MB each) between workers and main thread

**Current**: Structured cloning (copies data)
```javascript
worker.postMessage({ ciphertext }); // 1MB copy ‚Üí 10-20ms overhead
```

**Optimized**: SharedArrayBuffer (zero-copy)
```javascript
const sharedBuffer = new SharedArrayBuffer(5 * 1024 * 1024); // 5MB
const view = new Uint8Array(sharedBuffer);
worker.postMessage({ buffer: sharedBuffer }, [sharedBuffer]); // Instant!
```

**Implementation**:
```typescript
// frontend/src/services/sharedMemory.ts
export class SharedCiphertextBuffer {
  private buffer: SharedArrayBuffer;
  private offsets: number[];

  constructor(numFeatures: number, sizePerFeature: number) {
    this.buffer = new SharedArrayBuffer(numFeatures * sizePerFeature);
    this.offsets = new Array(numFeatures)
      .fill(0)
      .map((_, i) => i * sizePerFeature);
  }

  getView(index: number): Uint8Array {
    return new Uint8Array(
      this.buffer, 
      this.offsets[index], 
      this.offsets[index + 1] - this.offsets[index]
    );
  }

  getBuffer(): SharedArrayBuffer {
    return this.buffer;
  }
}
```

**Security Note**: Requires CORS headers
```
Cross-Origin-Opener-Policy: same-origin
Cross-Origin-Embedder-Policy: require-corp
```

**Expected Improvement**:
- Transfer time: 50ms ‚Üí <1ms (50x speedup)

---

#### 1.3 WASM SIMD for NTT Operations
**Problem**: Number Theoretic Transform (NTT) is compute-intensive

**Current**: Scalar operations
```go
// wasm/main.go (current)
for i := 0; i < n; i++ {
  result[i] = (a[i] * b[i]) % modulus
}
```

**Optimized**: WASM SIMD (128-bit vectors)
```go
// wasm/main.go (with SIMD)
import "golang.org/x/sys/cpu"

func nttSIMD(a, b []uint64, modulus uint64) []uint64 {
  if cpu.WASM.HasSIMD128 {
    // Process 2 uint64s at once (128-bit)
    for i := 0; i < len(a); i += 2 {
      vec_a := simd.Load128(a[i:])
      vec_b := simd.Load128(b[i:])
      vec_result := simd.Mul64x2(vec_a, vec_b)
      vec_result = simd.Mod64x2(vec_result, modulus)
      simd.Store128(result[i:], vec_result)
    }
  } else {
    // Fallback to scalar
  }
}
```

**Browser Support** (as of 2025):
- ‚úÖ Chrome 91+ (98% market share)
- ‚úÖ Firefox 89+
- ‚úÖ Edge 91+
- ‚ö†Ô∏è Safari 16.4+ (limited)

**Expected Improvement**:
- NTT performance: 2-3x speedup
- Overall encryption: 20-30% faster

---

### üîß Phase 2: Backend Optimization (Q2 2026)

#### 2.1 Batch Processing
**Use Case**: Multiple users submitting at the same time

**Current**: Process one request at a time
```go
// Sequential
for _, req := range requests {
  result := processRequest(req) // 425ms each
  sendResponse(result)
}
// 10 requests ‚Üí 4250ms
```

**Optimized**: Batch inference
```go
// Parallel (same as frontend workers concept)
func processBatch(requests []Request) []Response {
  results := make([]Response, len(requests))
  
  // Process in parallel (Go routines)
  var wg sync.WaitGroup
  for i, req := range requests {
    wg.Add(1)
    go func(idx int, r Request) {
      defer wg.Done()
      results[idx] = processRequest(r)
    }(i, req)
  }
  wg.Wait()
  
  return results
}
// 10 requests ‚Üí 425ms (10x speedup!)
```

**Trade-off**: CPU cores √ó Memory
- Requires multi-core server
- Each inference needs ~500MB RAM (CKKS ciphertexts)
- 8-core server ‚Üí 8 concurrent inferences

---

#### 2.2 Ciphertext Compression
**Problem**: 5 features √ó 1MB = 5MB network transfer

**Current**: Raw ciphertext serialization
```go
ct.MarshalBinary() // 1048919 bytes per feature
```

**Optimized**: LZ4 compression
```go
import "github.com/pierrec/lz4/v4"

func compressCiphertext(ct *rlwe.Ciphertext) ([]byte, error) {
  raw, err := ct.MarshalBinary()
  if err != nil {
    return nil, err
  }
  
  compressed := make([]byte, lz4.CompressBlockBound(len(raw)))
  n, err := lz4.CompressBlock(raw, compressed, nil)
  if err != nil {
    return nil, err
  }
  
  return compressed[:n], nil
}
```

**Expected Compression Ratio**:
- Raw: 1,048,919 bytes
- Compressed: ~300,000 bytes (70% reduction)
- Network time: 50ms ‚Üí 15ms (3.3x speedup)

---

#### 2.3 GPU Acceleration (Advanced)
**Use Case**: High-throughput production environment

**Libraries**:
- NVIDIA cuFHE (CUDA-accelerated FHE)
- Intel HE-Transformer (oneDNN backend)

**Implementation** (Conceptual):
```go
// Requires CGO + CUDA
// #cgo LDFLAGS: -lcufhe -lcudart
// import "C"

func performInferenceGPU(features []*rlwe.Ciphertext) (*rlwe.Ciphertext, error) {
  // Transfer to GPU
  d_features := cudaMemcpy(features)
  
  // GPU kernel for weighted sum
  d_result := cudaWeightedSum(d_features, model.Weights)
  
  // GPU sigmoid (if available)
  d_score := cudaSigmoid(d_result)
  
  // Transfer back to CPU
  return cudaMemcpyToHost(d_score)
}
```

**Expected Improvement**:
- Weighted sum: 20ms ‚Üí 2ms (10x speedup)
- Sigmoid: 365ms ‚Üí 50ms (7x speedup)
- Total backend: 425ms ‚Üí 75ms (5.6x speedup)

**Caveat**: Requires expensive hardware ($10,000+ NVIDIA A100)

---

### üì¶ Phase 3: Advanced Techniques (2027+)

#### 3.1 Client-Side Caching
**Use Case**: Repeat users or similar inputs

**Strategy**: Cache public keys and preprocessed features
```typescript
// frontend/src/services/cacheManager.ts
class CKKSCache {
  private publicKeyCache: Map<string, PublicKey>;
  private featureCache: Map<string, EncryptedFeature>;

  async getOrEncrypt(
    userId: string, 
    feature: number
  ): Promise<EncryptedFeature> {
    const cacheKey = `${userId}_${feature.toFixed(2)}`;
    
    if (this.featureCache.has(cacheKey)) {
      return this.featureCache.get(cacheKey)!;
    }
    
    const encrypted = await encrypt(feature);
    this.featureCache.set(cacheKey, encrypted);
    return encrypted;
  }
}
```

**Expected Improvement**:
- Cache hit: 0ms (instant)
- Cache miss: 200ms (same as before)
- 50% hit rate ‚Üí 100ms average (2x speedup)

---

#### 3.2 Progressive Encryption
**Use Case**: Show progress to user during long operations

**Implementation**:
```typescript
async function encryptWithProgress(
  features: number[],
  onProgress: (percent: number) => void
): Promise<EncryptedFeature[]> {
  const results: EncryptedFeature[] = [];
  
  for (let i = 0; i < features.length; i++) {
    results.push(await encrypt(features[i]));
    onProgress((i + 1) / features.length * 100);
  }
  
  return results;
}

// UI Component
<ProgressBar 
  value={encryptionProgress} 
  label={`Encrypting... ${encryptionProgress}%`}
/>
```

**User Experience**:
- Before: Blank screen for 1 second
- After: "Encrypting... 40%" feedback

---

#### 3.3 Pipelined Processing
**Use Case**: Overlap encryption, network, and backend computation

**Current**: Sequential stages
```
Encrypt (1000ms) ‚Üí Network (50ms) ‚Üí Backend (425ms) ‚Üí Decrypt (100ms)
Total: 1575ms
```

**Optimized**: Pipelined stages
```
Feature 1: Encrypt ‚Üí Send ‚Üí Process
Feature 2:  Encrypt ‚Üí Send ‚Üí Process
Feature 3:   Encrypt ‚Üí Send ‚Üí Process
Feature 4:    Encrypt ‚Üí Send ‚Üí Process
Feature 5:     Encrypt ‚Üí Send ‚Üí Process

Total: max(1000, 50, 425, 100) = 1000ms (1.5x speedup)
```

**Implementation** (Conceptual):
```typescript
async function pipelinedInference(features: number[]) {
  const pipeline = features.map(async (feature, i) => {
    const encrypted = await encrypt(feature);
    const response = await sendToBackend([encrypted]);
    return response;
  });
  
  const results = await Promise.all(pipeline);
  const aggregated = aggregateResults(results);
  const decrypted = await decrypt(aggregated);
  
  return decrypted;
}
```

**Caveat**: Requires backend API changes (stream processing)

---

## Performance Roadmap

| Phase | Optimization | Current | Target | Timeline |
|-------|-------------|---------|--------|----------|
| **0** | Baseline | 1575ms | - | Now |
| **1A** | Web Workers | 1575ms | 775ms | Q1 2026 |
| **1B** | SIMD | 775ms | 620ms | Q1 2026 |
| **2A** | Ciphertext Compression | 620ms | 585ms | Q2 2026 |
| **2B** | Backend Batching | 585ms | 585ms* | Q2 2026 |
| **3A** | Client Caching | 585ms | 292ms** | Q3 2026 |
| **3B** | GPU Acceleration | 292ms | 192ms | 2027+ |

\* Batching improves throughput, not latency per request  
\** With 50% cache hit rate

---

## Implementation Priority

### High Priority (Q1 2026)
1. ‚úÖ **Web Workers**: Easiest, biggest impact (2.2x speedup)
2. ‚úÖ **Ciphertext Compression**: Low effort, good gain (1.06x speedup)
3. ‚úÖ **Progress UI**: Improves perceived performance

### Medium Priority (Q2 2026)
4. ‚è∏Ô∏è **WASM SIMD**: Requires browser compatibility checks
5. ‚è∏Ô∏è **Backend Batching**: Only useful for multi-user scenarios
6. ‚è∏Ô∏è **Pipelined Processing**: Requires architecture refactor

### Low Priority (2027+)
7. üîÆ **GPU Acceleration**: Expensive hardware, complex setup
8. üîÆ **Custom Sigmoid**: Requires cryptographic expertise

---

## Risk Assessment

### Web Workers
- **Risk**: Browser compatibility (97% support in 2025)
- **Mitigation**: Fallback to sequential encryption
- **Test**: Feature detection with `typeof Worker !== 'undefined'`

### SharedArrayBuffer
- **Risk**: CORS policy issues (requires HTTPS + headers)
- **Mitigation**: Use without SAB (still 5x speedup from workers)
- **Test**: Check `typeof SharedArrayBuffer !== 'undefined'`

### WASM SIMD
- **Risk**: Safari support incomplete (16% market share)
- **Mitigation**: Runtime detection + fallback
- **Test**: `cpu.WASM.HasSIMD128`

### GPU Acceleration
- **Risk**: High cost, complex deployment
- **Mitigation**: Only for high-volume production (>1000 req/sec)
- **Test**: Benchmark on dev server first

---

## Code Examples

### Complete Web Worker Implementation

**File: `frontend/src/workers/ckks.worker.ts`**
```typescript
import { loadWASM } from '../services/wasmLoader';

let wasmModule: any = null;

self.onmessage = async (e: MessageEvent) => {
  const { cmd, data } = e.data;

  // Initialize WASM on first call
  if (!wasmModule) {
    wasmModule = await loadWASM();
  }

  switch (cmd) {
    case 'encrypt':
      const { value, publicKey } = data;
      const encrypted = wasmModule.encryptValue(value, publicKey);
      self.postMessage({ cmd: 'encrypt', result: encrypted });
      break;

    case 'decrypt':
      const { ciphertext, secretKey } = data;
      const decrypted = wasmModule.decryptValue(ciphertext, secretKey);
      self.postMessage({ cmd: 'decrypt', result: decrypted });
      break;

    default:
      self.postMessage({ cmd: 'error', error: 'Unknown command' });
  }
};
```

**File: `frontend/src/services/workerPool.ts`**
```typescript
export class WorkerPool {
  private workers: Worker[] = [];
  private queue: Array<{ resolve: Function; reject: Function; task: any }> = [];
  private busy: boolean[] = [];

  constructor(size: number, workerScript: string) {
    for (let i = 0; i < size; i++) {
      const worker = new Worker(new URL(workerScript, import.meta.url));
      worker.onmessage = (e) => this.handleMessage(i, e);
      this.workers.push(worker);
      this.busy[i] = false;
    }
  }

  async execute<T>(task: any): Promise<T> {
    return new Promise((resolve, reject) => {
      const freeWorker = this.busy.findIndex(b => !b);
      
      if (freeWorker !== -1) {
        this.busy[freeWorker] = true;
        this.workers[freeWorker].postMessage(task);
        this.queue.push({ resolve, reject, task });
      } else {
        this.queue.push({ resolve, reject, task });
      }
    });
  }

  private handleMessage(workerId: number, e: MessageEvent) {
    this.busy[workerId] = false;
    const { resolve } = this.queue.shift()!;
    resolve(e.data.result);

    // Process next queued task
    if (this.queue.length > 0) {
      const next = this.queue[0];
      this.busy[workerId] = true;
      this.workers[workerId].postMessage(next.task);
    }
  }

  terminate() {
    this.workers.forEach(w => w.terminate());
  }
}
```

**Usage in React Component**:
```typescript
// frontend/src/contexts/FHEContext.tsx
const workerPool = useMemo(() => 
  new WorkerPool(5, '../workers/ckks.worker.ts'), 
  []
);

const encryptFeatures = async (features: number[]) => {
  const tasks = features.map(value => ({
    cmd: 'encrypt',
    data: { value, publicKey: keys.publicKey }
  }));

  const results = await Promise.all(
    tasks.map(task => workerPool.execute(task))
  );

  return results;
};
```

---

## Monitoring & Metrics

### Performance Tracking
```typescript
// frontend/src/services/metrics.ts
export class PerformanceMetrics {
  private metrics: Map<string, number[]> = new Map();

  record(operation: string, durationMs: number) {
    if (!this.metrics.has(operation)) {
      this.metrics.set(operation, []);
    }
    this.metrics.get(operation)!.push(durationMs);
  }

  getStats(operation: string) {
    const data = this.metrics.get(operation) || [];
    return {
      mean: data.reduce((a, b) => a + b, 0) / data.length,
      median: data.sort()[Math.floor(data.length / 2)],
      p95: data.sort()[Math.floor(data.length * 0.95)],
      p99: data.sort()[Math.floor(data.length * 0.99)]
    };
  }

  exportJSON() {
    const report: any = {};
    for (const [op, data] of this.metrics) {
      report[op] = this.getStats(op);
    }
    return JSON.stringify(report, null, 2);
  }
}

// Usage
const metrics = new PerformanceMetrics();

const start = performance.now();
await encryptFeatures(features);
metrics.record('encryption', performance.now() - start);

console.log(metrics.getStats('encryption'));
// { mean: 823.4, median: 805.2, p95: 1023.1, p99: 1198.7 }
```

---

## Conclusion

**Immediate Actions** (Q1 2026):
1. Implement Web Workers for parallel encryption
2. Add LZ4 compression to reduce network time
3. Add progress indicators for better UX

**Expected Results**:
- Latency: 1575ms ‚Üí 620ms (2.5x speedup)
- User satisfaction: üìà "Faster than expected!"
- Infrastructure cost: Same (no GPU needed yet)

**Next Review**: Q2 2026 - Evaluate SIMD and batching performance
